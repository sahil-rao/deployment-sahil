---
- name: Create the Baaz config directory
  file: path=/var/Baaz state=directory
  sudo: yes
- name: NTP installed
  apt: name=ntp state=present
  sudo: yes
- name: Save configuration file
  template: src=hosts.cfg dest=/var/Baaz/hosts.cfg
  sudo: yes
- name: Get Flightpath
  shell: s3cmd sync s3://baaz-deployment/{{ BuildVersion }}flightpath-deployment.tar.gz .
- name: Untar flightpath
  shell: tar -zxf /home/ubuntu/flightpath-deployment.tar.gz chdir=/
  sudo: yes
- name: Get Data Acquisition service
  shell: s3cmd sync s3://baaz-deployment/{{ BuildVersion }}Baaz-DataAcquisition-Service.tar.gz .
- name: Copy the service files
  shell: tar -zxf /home/ubuntu/Baaz-DataAcquisition-Service.tar.gz chdir=/
  sudo: yes
- name: Kill Monit so it won't interfere with installation
  shell: monit quit
  sudo: yes
- name: Remove old monit ID
  shell: rm -f /var/lib/monit/id
  sudo: yes
- name: Remove any previous service that may be installed.
  shell: update-rc.d -f baazdataacquisition remove
  sudo: yes
- name: Stop running upstart script
  service: name=dataacquisitionservice state=stopped
  sudo: yes
- name: Stop running upstart script
  service: name=applicationservice state=stopped
  sudo: yes
# - name: Start dataacquisitionservice (upstart).
#   service: name=dataacquisitionservice state=started
#   sudo: yes
- name: copy logrotate file
  template: src=roles/backoffice/files/compileserver dest=/etc/logrotate.d/
  sudo: yes

#Start of Analytics module deployment.
- name: Get Analytics Module
  shell: s3cmd sync s3://baaz-deployment/{{ BuildVersion }}Baaz-Analytics.tar.gz .
- name: Untar Analytics
  shell: tar -zxf /home/ubuntu/Baaz-Analytics.tar.gz chdir=/
  sudo: yes
- name: Get Analytics Service
  shell: s3cmd sync s3://baaz-deployment/{{ BuildVersion }}Baaz-Analytics-Service.tar.gz .
- name: Get Basestats 
  shell: s3cmd sync s3://baaz-deployment/{{ BuildVersion }}Baaz-Basestats-Report.tar.gz .
- name: Install Basestats scripts
  shell: tar -zxf /home/ubuntu/Baaz-Basestats-Report.tar.gz chdir=/usr/lib
  sudo: yes
- name: Install Analytics Service
  shell: tar -zxf /home/ubuntu/Baaz-Analytics-Service.tar.gz chdir=/
  sudo: yes
- name: Remove any previous service that may be installed.
  shell: update-rc.d -f baazmath remove
  sudo: yes
- name: Stop mathservice.
  service: name=mathservice state=stopped
  sudo: yes
- name: Stop advanalytics.
  service: name=advanalytics state=stopped
  sudo: yes
# - name: Start mathservice (upstart).
#   service: name=mathservice state=started
#   sudo: yes

- name: Get Compiler Module
  shell: s3cmd sync s3://baaz-deployment/{{ BuildVersion }}Baaz-Compiler.tar.gz .
- name: Remove the existing compiler deployed code
  file: path=/usr/lib/baaz_compiler state=absent
  sudo: yes
- name: Untar Compiler binaries
  shell: tar -zxf /home/ubuntu/Baaz-Compiler.tar.gz chdir=/usr/lib
  sudo: yes
- name: Get Compiler Service
  shell: s3cmd sync s3://baaz-deployment/{{ BuildVersion }}Baaz-Compile-Service.tar.gz .
- name: Install Compiler Service
  shell: tar -zxf /home/ubuntu/Baaz-Compile-Service.tar.gz chdir=/
  sudo: yes
- name: Remove any previous service that may be installed.
  shell: update-rc.d -f baazcompile remove
  sudo: yes
- name: Remove any previous xplaincompile server that may be installed.
  service: name=xplaincompile state=stopped
  ignore_errors: yes
  sudo: yes
- name: Delete xplaincompile.conf
  shell: rm -f /etc/init/xplaincompile.conf
  sudo: yes
- name: Stop compileservice.
  service: name=compileservice state=stopped
  sudo: yes
- name: Stop compileserver.
  service: name=compileserver state=stopped
  sudo: yes
# - name: Start compileserver.
#   service: name=compileserver state=started
#   sudo: yes
# - name: Start compileservice.
#   service: name=compileservice state=started
#   sudo: yes
- name: copy monitrc file
  template: src=roles/backoffice/files/monitrc dest=/etc/monit/monitrc
  sudo: yes
- name: Start hive server
  action: service name=hive state=started
  sudo: yes
- name: Install Elasticsearch
  pip: name=elasticsearch
  sudo: yes

#REDIS SENTINEL DEPLOYMENT
- name: Install Redis APT Package
  apt: name=redis-server update_cache=yes
  sudo: yes
- name: Stop running redis server
  service: name=redis-server state=stopped
  sudo: yes
- name: disable redis-server from init.d
  shell: update-rc.d -f redis-server disable
  sudo: yes
- name: remove redis-server from init.d
  shell: update-rc.d -f redis-server remove
  sudo: yes
- name: Create redis config directory
  file: path=/etc/redis/local owner=redis group=redis mode=777 state=directory
  sudo: yes
- name: Create redis config directory
  file: path=/var/log/sentinel owner=redis group=redis mode=777 state=directory
  sudo: yes
- name: Copy sentinel config file
  template: src=roles/backoffice/templates/sentinel.conf dest=/etc/redis/local/sentinel.conf owner=redis group=redis mode=0644
  sudo: yes
- name: Write upstart script in /etc/init/redis-sentinel.conf
  copy: src=roles/backoffice/files/redis-sentinel.conf dest=/etc/init/redis-sentinel.conf 
  sudo: yes
- name: Start sentinel
  service: name=redis-sentinel state=started
  sudo: yes

- name: Start monit service
  shell: monit 
  sudo: yes
# - name: Reload monit
#   shell: monit reload
#   sudo: yes
# - name: Use monit to start all services
#   shell: monit start all
#   sudo: yes

#uncomment below change for impala enablement

#Impala configuration related changes
- name: Install Java repo
  command: add-apt-repository -y ppa:webupd8team/java
  sudo: yes
- name: Run the update
  command: apt-get update
  sudo: yes
  ignore_errors: True
- name: Update config for java update
  shell: echo oracle-java7-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections
- name: Install java version
  command: apt-get install -y oracle-jdk7-installer
  sudo: yes
- name: Set JAVA HOME
  shell: JAVA_HOME="/usr/lib/jvm/java-7-oracle"
  sudo: yes
- name: Change the permission of /etc/enviornment file
  command: chmod 666 /etc/environment
  sudo: yes
- name: update /etc/enviornment file with java info
  command: echo "JAVA_HOME=\"/usr/lib/jvm/java-7-oracle\"" >> /etc/environment
  sudo: yes

- name: Get Impala Module
  shell: s3cmd sync s3://baaz-deployment/{{ BuildVersion }}Impala.tar.gz .
  sudo: yes
- name: Untar Impala prebuild code
  shell: tar -zxf /home/ubuntu/Impala.tar.gz
- name: Remove old Hive from /usr/local
  shell: rm -rf /usr/local/hive
  sudo: yes
  ignore_errors: True
- name: Copy over Hive to /usr/local
  command: cp -R /home/ubuntu/Impala/thirdparty/hive-0.13.1-cdh5.3.0 /usr/local/
  sudo: yes
- name: Rename Hive in /usr/local
  command: mv /usr/local/hive-0.13.1-cdh5.3.0 /usr/local/hive
  sudo: yes
- name: Remove old Hadoop from /usr/local
  shell: rm -rf /usr/local/hadoop
  sudo: yes
- name: Copy over Hadoop to /usr/local
  command: cp -R /home/ubuntu/Impala/thirdparty/hadoop-2.5.0-cdh5.3.0 /usr/local/
  sudo: yes
- name: Rename Hadoop in /usr/local
  command: mv /usr/local/hadoop-2.5.0-cdh5.3.0 /usr/local/hadoop
  sudo: yes
- name: Install sasl dependency..
  command: apt-get install python-dev libsasl2-dev gcc g++
  sudo: yes
  ignore_errors: True
- name: Install sasl
  pip: name=sasl
  sudo: yes
  ignore_errors: True
- name: Install thrift
  pip: name=sasl
  sudo: yes
  ignore_errors: True
- name: Install impyla
  pip: name=impyla
  sudo: yes
  ignore_errors: True
- name: Install pyhs2
  pip: name=pyhs2
  sudo: yes
  ignore_errors: True

- name: Create a directory for Hadoop configuration files
  shell: mkdir -p /etc/hadoop/conf
  sudo: yes
- name: Create a directory for Hive configuration files
  shell: mkdir -p /etc/hive/conf
  sudo: yes
- name: Create a directory for Impala configuration files
  shell: mkdir -p /etc/impala/conf
  sudo: yes

#change permission if not accurate
- name: Update Hadoop permissions
  shell: chown -R ubuntu:root /usr/local/hadoop
  sudo: yes
- name: Update Hive permissions
  shell: chown -R ubuntu:root /usr/local/hive
  sudo: yes
- name: Update Hadoop daemon permission
  shell: chmod 777 /usr/local/hadoop/sbin/hadoop-daemon.sh
  sudo: yes
- name: Update Hadoop yarn daemon permission
  shell: chmod 755 /usr/local/hadoop/sbin/yarn-daemon.sh
  sudo: yes
- name: Update binary permissions
  shell: chmod 755 /usr/local/hadoop/bin/hdfs
  sudo: yes
- name: Update binary permissions
  shell: chmod 755 /usr/local/hadoop/bin/hadoop
  sudo: yes
- name: Update binary permissions
  shell: chmod 755 /usr/local/hadoop/bin/yarn
  sudo: yes
- name: Update binary permissions
  shell: chmod 755 /usr/local/hive/bin/hive
  sudo: yes
- name: Update root permissions
  shell: sudo chmod 555 /
  sudo: yes
- name: Create a yarn directory
  shell: mkdir -p /var/lib/hadoop-yarn
  sudo: yes
- name: Create a hadoop directory
  shell: mkdir -p /var/lib/hadoop-hdfs
  sudo: yes
- name: Update Hadoop dir permissions
  shell: chown -R ubuntu:root /var/lib/hadoop-hdfs
  sudo: yes
- name: Create a hadoop directory
  shell: mkdir -p /var/run/hadoop-hdfs
  sudo: yes
- name: Update Hadoop dir permissions
  shell: chown -R ubuntu:root /var/run/hadoop-hdfs
  sudo: yes
- name: Install mysql jar files
  shell: apt-get install libmysql-java
  sudo: yes
- name: create a symlink for mysql jar file
  shell: cp /usr/share/java/mysql-connector-java-5.1.28.jar /usr/local/hive/lib/
  sudo: yes
- name: copy boost lib
  shell: cp /home/ubuntu/Impala/libboost_thread.so.1.46.1 /usr/lib/
  sudo: yes
- name: copy boost lib
  shell: cp /home/ubuntu/Impala/libboost_regex.so.1.46.1 /usr/lib/
  sudo: yes
- name: copy boost lib
  shell: cp /home/ubuntu/Impala/libboost_system.so.1.46.1 /usr/lib/
  sudo: yes
- name: copy boost lib
  shell: cp /home/ubuntu/Impala/libboost_filesystem.so.1.46.1 /usr/lib/
  sudo: yes
- name: copy boost lib
  shell: cp /home/ubuntu/Impala/libboost_date_time.so.1.46.1 /usr/lib/
  sudo: yes
- name: copy icui lib
  shell: cp /home/ubuntu/Impala/libicudata.so.48 /usr/lib/
  sudo: yes
- name: copy icui lib
  shell: cp /home/ubuntu/Impala/libicui18n.so.48 /usr/lib/
  sudo: yes
- name: copy icui lib
  shell: cp /home/ubuntu/Impala/libicuuc.so.48 /usr/lib/
  sudo: yes
- name: copy hdfs lib
  shell: cp /home/ubuntu/Impala/libhdfs.so.0.0.0 /usr/lib/
  sudo: yes

#mysql configuration for hive metastore
- name: Install python module for mysql
  shell: apt-get install python-mysqldb
  sudo: yes
- name: Set root password for mysql
  shell: mysqladmin -u root password mysql
  sudo: yes
  ignore_errors: True
- name: create a database metastore in mysql
  mysql_db: name=metastore login_user=root login_password=mysql state=present
- name: feed database.sql to server
  mysql_db: login_user=root login_password=mysql name=metastore state=import target=/usr/local/hive/scripts/metastore/upgrade/mysql/hive-schema-0.13.0.mysql.sql
  sudo: yes
  ignore_errors: True
- name: create working directory for Hive
  shell: mkdir -p /var/run/hive
  sudo: yes
- name: create working directory for Hive
  shell: mkdir -p /var/log/hive
  sudo: yes
- name: Change the permission of dir
  shell: chown -R ubuntu:root /var/run/hive
  sudo: yes
- name: Change the permission of dir
  shell: chown -R ubuntu:root /var/log/hive
  sudo: yes
- name: create working directory for Impala
  shell: mkdir -p /var/run/impala
  sudo: yes
- name: Change the permission of dir
  shell: chown -R ubuntu:root /var/run/impala
  sudo: yes
- name: create working directory for Impala
  shell: mkdir -p /var/log/impala
  sudo: yes
- name: Change the permission of dir
  shell: chown -R ubuntu:root /var/log/impala
  sudo: yes

- name: Copy over Hadoop core configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/core-site.xml /etc/hadoop/conf/
  sudo: yes
- name: Copy over Hadoop core configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/core-site.xml /etc/impala/conf/
  sudo: yes
- name: Copy over Hadoop core configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/core-site.xml /etc/hive/conf/
  sudo: yes
- name: Copy over Hadoop core configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/core-site.xml /usr/local/hadoop/etc/hadoop
  sudo: yes
- name: Copy over Hadoop hdfs configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/hdfs-site.xml /etc/hadoop/conf/
  sudo: yes
- name: Copy over Hadoop hdfs configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/hdfs-site.xml /etc/impala/conf/
  sudo: yes
- name: Copy over Hadoop hdfs configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/hdfs-site.xml /etc/hive/conf/
  sudo: yes
- name: Copy over Hadoop hdfs configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/hdfs-site.xml /usr/local/hadoop/etc/hadoop
  sudo: yes
- name: Copy over Hadoop yarn configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/yarn-site.xml /etc/hadoop/conf/
  sudo: yes
- name: Copy over Hadoop yarn configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/yarn-site.xml /etc/impala/conf/
  sudo: yes
- name: Copy over Hadoop yarn configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/yarn-site.xml /etc/hive/conf/
  sudo: yes
- name: Copy over Hadoop yarn configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/yarn-site.xml /usr/local/hadoop/etc/hadoop
  sudo: yes
- name: Copy over Hadoop mapred configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/mapred-site.xml /etc/hadoop/conf/
  sudo: yes
- name: Copy over Hadoop mapred configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/mapred-site.xml /etc/impala/conf/
  sudo: yes
- name: Copy over Hadoop mapred configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/mapred-site.xml /etc/hive/conf/
  sudo: yes
- name: Copy over Hadoop mapred configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/mapred-site.xml /usr/local/hadoop/etc/hadoop
  sudo: yes
- name: Copy over Hadoop ENV file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/hadoop-env.sh /etc/hadoop/conf/
  sudo: yes
- name: Copy over Hadoop ENV file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/hadoop-env.sh /etc/impala/conf/
  sudo: yes
- name: Copy over Hadoop ENV file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/hadoop-env.sh /etc/hive/conf/
  sudo: yes
- name: Copy over Hadoop ENV file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/hadoop-env.sh /usr/local/hadoop/etc/hadoop
  sudo: yes
- name: Copy over Hive configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/hive-site.xml /etc/hive/conf/
  sudo: yes
- name: Copy over Hive configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/hive-site.xml /etc/hadoop/conf/
  sudo: yes
- name: Copy over Hive configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/hive-site.xml /etc/impala/conf/
  sudo: yes
- name: Copy over Hive configuration file
  command: cp /home/ubuntu/Impala/fe/src/test/resources/hive-site.xml /usr/local/hive/conf/
  sudo: yes
- name: Copy over Impala state store binary to /usr/bin
  command: cp /home/ubuntu/Impala/be/build/debug/statestore/statestored /usr/bin/statestored
  sudo: yes
  ignore_errors: True
- name: Copy over Impala catalog binary to /usr/bin
  command: cp /home/ubuntu/Impala/be/build/debug/catalog/catalogd /usr/bin/catalogd
  sudo: yes
  ignore_errors: True
- name: Copy over Impala server binary to /usr/bin
  command: cp /home/ubuntu/Impala/be/build/debug/service/impalad /usr/bin/impalad
  sudo: yes
  ignore_errors: True
- name: Update binary permissions
  shell: chmod 755 /usr/bin/impalad
  sudo: yes
  ignore_errors: True
- name: Copy over Impala config to /etc/default
  command: cp /home/ubuntu/Impala/impala /etc/default/
  sudo: yes
  ignore_errors: True

#copy system start up scripts
- name: Copy over System V files into /etc/init.d/
  command: cp /home/ubuntu/Impala/hadoop-hdfs-datanode /etc/init.d/hadoop-hdfs-datanode
  sudo: yes
- name: Copy over System V files into /etc/init.d/
  command: cp /home/ubuntu/Impala/hadoop-hdfs-namenode /etc/init.d/hadoop-hdfs-namenode
  sudo: yes
- name: Copy over System V files into /etc/init.d/
  command: cp /home/ubuntu/Impala/hadoop-hdfs-secondarynamenode /etc/init.d/hadoop-hdfs-secondarynamenode
  sudo: yes
- name: Copy over System V files into /etc/init.d/
  command: cp /home/ubuntu/Impala/hadoop-yarn-nodemanager /etc/init.d/hadoop-yarn-nodemanager
  sudo: yes
- name: Copy over System V files into /etc/init.d/
  command: cp /home/ubuntu/Impala/hadoop-yarn-resourcemanager /etc/init.d/hadoop-yarn-resourcemanager
  sudo: yes
- name: Copy over System V files into /etc/init.d/
  command: cp /home/ubuntu/Impala/hive-metastore /etc/init.d/hive-metastore
  sudo: yes
- name: Copy over System V files into /etc/init.d/
  command: cp /home/ubuntu/Impala/impala-catalog /etc/init.d/impala-catalog
  sudo: yes
- name: Copy over System V files into /etc/init.d/
  command: cp /home/ubuntu/Impala/impala-server /etc/init.d/impala-server
  sudo: yes
- name: Copy over System V files into /etc/init.d/
  command: cp /home/ubuntu/Impala/impala-state-store /etc/init.d/impala-state-store
  sudo: yes

#create a symlinks for that points to above start up script
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-hdfs-namenode S85hadoop-hdfs-namenode chdir=/etc/rc2.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-hdfs-datanode S85hadoop-hdfs-datanode chdir=/etc/rc2.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-hdfs-secondarynamenode S85hadoop-hdfs-secondarynamenode chdir=/etc/rc2.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-yarn-resourcemanager S85hadoop-yarn-resourcemanager chdir=/etc/rc2.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-yarn-nodemanager S85hadoop-yarn-nodemanager chdir=/etc/rc2.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hive-metastore S85hive-metastore chdir=/etc/rc2.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hive-server2 S85hive-server2 chdir=/etc/rc2.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/impala-state-store S90impala-state-store chdir=/etc/rc2.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/impala-server S91impala-server chdir=/etc/rc2.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/impala-catalog S92impala-catalog chdir=/etc/rc2.d
  ignore_errors: True

- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-hdfs-namenode S85hadoop-hdfs-namenode chdir=/etc/rc3.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-hdfs-datanode S85hadoop-hdfs-datanode chdir=/etc/rc3.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-hdfs-secondarynamenode S85hadoop-hdfs-secondarynamenode chdir=/etc/rc3.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-yarn-resourcemanager S85hadoop-yarn-resourcemanager chdir=/etc/rc3.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-yarn-nodemanager S85hadoop-yarn-nodemanager chdir=/etc/rc3.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hive-metastore S85hive-metastore chdir=/etc/rc3.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hive-server2 S85hive-server2 chdir=/etc/rc3.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/impala-state-store S90impala-state-store chdir=/etc/rc3.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/impala-server S91impala-server chdir=/etc/rc3.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/impala-catalog S92impala-catalog chdir=/etc/rc3.d
  ignore_errors: True

- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-hdfs-namenode S85hadoop-hdfs-namenode chdir=/etc/rc4.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-hdfs-datanode S85hadoop-hdfs-datanode chdir=/etc/rc4.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-hdfs-secondarynamenode S85hadoop-hdfs-secondarynamenode chdir=/etc/rc4.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-yarn-resourcemanager S85hadoop-yarn-resourcemanager chdir=/etc/rc4.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-yarn-nodemanager S85hadoop-yarn-nodemanager chdir=/etc/rc4.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hive-metastore S85hive-metastore chdir=/etc/rc4.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hive-server2 S85hive-server2 chdir=/etc/rc4.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/impala-state-store S90impala-state-store chdir=/etc/rc4.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/impala-server S91impala-server chdir=/etc/rc4.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/impala-catalog S92impala-catalog chdir=/etc/rc4.d
  ignore_errors: True

- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-hdfs-namenode S85hadoop-hdfs-namenode chdir=/etc/rc5.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-hdfs-datanode S85hadoop-hdfs-datanode chdir=/etc/rc5.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-hdfs-secondarynamenode S85hadoop-hdfs-secondarynamenode chdir=/etc/rc5.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-yarn-resourcemanager S85hadoop-yarn-resourcemanager chdir=/etc/rc5.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hadoop-yarn-nodemanager S85hadoop-yarn-nodemanager chdir=/etc/rc5.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hive-metastore S85hive-metastore chdir=/etc/rc5.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/hive-server2 S85hive-server2 chdir=/etc/rc5.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/impala-state-store S90impala-state-store chdir=/etc/rc5.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/impala-server S91impala-server chdir=/etc/rc5.d
  ignore_errors: True
- name: Create a symoblic link for all Hadoop, Hive and Impala process
  sudo: yes
  command: ln -s ../init.d/impala-catalog S92impala-catalog chdir=/etc/rc5.d
  ignore_errors: True

- name: Create Bigtop directory
  command: mkdir -p /usr/lib/bigtop-utils
  sudo: yes
- name: Copy Bigtop dependency files
  command: cp /home/ubuntu/Impala/bigtop-detect-classpath /usr/lib/bigtop-utils/
  sudo: yes
- name: Copy Bigtop dependency files
  command: cp /home/ubuntu/Impala/bigtop-detect-javahome /usr/lib/bigtop-utils/
  sudo: yes
- name: Copy Bigtop dependency files
  command: cp /home/ubuntu/Impala/bigtop-detect-javalibs /usr/lib/bigtop-utils/
  sudo: yes
- name: Copy Bigtop dependency files
  command: cp /home/ubuntu/Impala/jsvc /usr/lib/bigtop-utils/
  sudo: yes

#delete hdfs directory
- name: Delete hdfs namenode dir
  command: rm -rf /home/ubuntu/Impala/testdata/cluster/cdh5/node-1/data/dfs/nn
  sudo: yes
- name: Delete hdfs datanode dir
  command: rm -rf /home/ubuntu/Impala/testdata/cluster/cdh5/node-1/data/dfs/dn
  sudo: yes
- name: format hdfs namenode
  command: /usr/local/hadoop/bin/hdfs namenode -format

- name: Stop Hadoop namenode
  command: pkill -f "namenode"
  ignore_errors: True
  sudo: yes
- name: Start Hadoop namenode
  service: name=hadoop-hdfs-namenode state=started
  ignore_errors: True
  sudo: yes
- name: Stop Hadoop datanode
  command: pkill -f "datanode"
  ignore_errors: True
  sudo: yes
- name: Start Hadoop datanode
  service: name=hadoop-hdfs-datanode state=started
  ignore_errors: True
  sudo: yes
- name: Stop Hadoop secondarynamenode
  command: pkill -f "secondarynamenode"
  ignore_errors: True
  sudo: yes
- name: Start Hadoop secondarynamenode
  service: name=hadoop-hdfs-secondarynamenode state=started
  ignore_errors: True
  sudo: yes
- name: Stop Hadoop yarn resourcemanager
  command: pkill -f "resourcemanager"
  ignore_errors: True
  sudo: yes
- name: Start Hadoop yarn resourcemanager
  service: name=hadoop-yarn-resourcemanager state=started
  ignore_errors: True
  sudo: yes
- name: Stop Hadoop yarn nodemanager
  command: pkill -f "nodemanager"
  ignore_errors: True
  sudo: yes
- name: Start Hadoop yarn nodemanager
  service: name=hadoop-yarn-nodemanager state=started
  ignore_errors: True
  sudo: yes
- name: Stop Hive Metastore
  service: name=hive-metastore state=stopped
  ignore_errors: True
  sudo: yes
- name: Start Hive Metastore
  service: name=hive-metastore state=started
  ignore_errors: True
  sudo: yes
- name: Stop Hive Server2
  service: name=hive-server2 state=stopped
  ignore_errors: True
  sudo: yes
- name: Start Hive Server2
  service: name=hive-server2 state=started
  ignore_errors: True
  sudo: yes
- name: Stop Impala Statestore
  service: name=impala-state-store state=stopped
  ignore_errors: True
  sudo: yes
- name: Start Impala Statestore
  service: name=impala-state-store state=started
  ignore_errors: True
  sudo: yes
- name: Stop Impala Server
  service: name=impala-server state=stopped
  ignore_errors: True
  sudo: yes
- name: Start Impala Server
  service: name=impala-server state=started
  ignore_errors: True
  sudo: yes
- name: Stop Impala Catalog
  service: name=impala-catalog state=stopped
  ignore_errors: True
  sudo: yes
- name: Start Impala Catalog
  service: name=impala-catalog state=started
  ignore_errors: True
  sudo: yes

