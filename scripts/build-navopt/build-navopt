#!/usr/bin/env python

import argparse
import getpass
import os
import subprocess
import sys
import boto3
from botocore.exceptions import ClientError
import time
import threading


class FuncThread(threading.Thread):
    def __init__(self, target, **kwargs):
        self._target = target
        self._kwargs = kwargs
        threading.Thread.__init__(self)

    def run(self):
       # print self._kwargs
        self._target(**self._kwargs['kwargs'])

CONFIG = {
    'dev': {
        'account': '128669107540',
        'aws_profile': 'navopt_dev',
        'region': 'us-west-2',
        'bucket': 'navopt-dev-deployment-bucket',
        'cluster': 'navopt-dev',
    },
    'alpha': {
        'account': '892272719698',
        'aws_profile': 'navopt_alpha',
        'region': 'us-west-1',
        'bucket': 'navopt-alpha-deployment-bucket',
        'cluster': 'navopt-alpha',
    },
    'stage': {
        'account': '001209911431',
        'aws_profile': 'navopt_stage',
        'region': 'us-west-2',
        'bucket': 'navopt-stage-deployment-bucket',
        'cluster': 'navopt-stage',
    },
    'prod': {
        'account': '892272719698',
        'aws_profile': 'navopt_prod',
        'region': 'us-west-2',
        'bucket': 'navopt-prod-deployment-bucket',
        'cluster': 'navopt-prod',
    },
}

# List out the services and their artifacts.
SERVICES = {
    'deployment': {
        'tarballs': [
            'Baaz-DataAcquisition-Service.tar.gz',
            'Baaz-Compile-Service.tar.gz',
            'Baaz-Analytics-Service.tar.gz',
            'Baaz-API-Service.tar.gz'
        ],
    },
    'UI': {
        'dependencies': ['deployment'],
        'tarballs': [
            'xplain.io.tar.gz',
            'optimizer_api.io.tar.gz',
            'xplain_dashboard.tar.gz',
            'optimizer_admin.io.tar.gz',
        ],
    },
    'analytics': {
        'dependencies': ['deployment'],
        'tarballs': [
            'Baaz-Analytics.tar.gz',
        ],
    },
    'compiler': {
        'dependencies': ['deployment'],
        'tarballs': [
            'Baaz-Compiler.tar.gz',
        ],
    },
    'graph': {
        'dependencies': ['deployment'],
        'tarballs': [
            'flightpath-deployment.tar.gz',
            'regression-deployment.tar.gz',
        ],
    },
    'navopt-pyservices': {
        'docker': True,
        'dependencies': [
            'deployment',
            'graph',
            'analytics',
        ],
    },
    'navopt-javaservices': {
        'docker': True,
        'dependencies': [
            'navopt-pyservices',
            'compiler',
        ],
        'ecs_tasks': [
            'navopt-applicationservice-test',
            'navopt-advanalytics-test',
            'navopt-mathservice-test',
            'navopt-ruleengine-test',
            'navopt-compileservice-test',
            'navopt-fpprocessing-test',
            'navopt-apiservice-test',
            'navopt-dataacquisitionservice-test'
        ]
    }
}

DEFAULT_GITHUB_PROJECT = 'git@github.com:baazdata'


def run_command(args, cmd, **kwargs):
    if args.verbose:
        print '+', ' '.join(cmd)

    if not args.dry_run:
        subprocess.check_call(cmd, **kwargs)


def checkout_repo(args, service):
    """
    Check out the service repo into a cache.
    """

    repo_base = args.repo
    if not repo_base.endswith('/'):
        repo_base += '/'

    repo_url = '{}{}.git'.format(repo_base, service)
    repo_dir = os.path.join('.gitcache', service)

    if os.path.exists(repo_dir):
        print '--- updating', repo_url

        cmd = [
            'git',
            '-C', repo_dir,
            'remote',
            'set-url',
            'origin',
            repo_url,
        ]
        run_command(args, cmd)

        cmd = [
            'git',
            '-C', repo_dir,
            'remote',
            'update',
            'origin',
        ]
        run_command(args, cmd)
    else:
        print '--- cloning', repo_url

        cmd = [
            'git',
            'clone',
            '--mirror',
            repo_url,
            repo_dir,
        ]
        run_command(args, cmd)


def build_service(args, service):
    """
    Build the service in a docker container.
    """

    checkout_repo(args, service)

    print '--- building', service

    cmd = [
        'docker',
        'build',
        '-t',
        'build-navopt',
        'scripts',
    ]
    run_command(args, cmd)

    # Run the docker build.
    cmd = [
        'docker',
        'run',
        '-v', os.path.expanduser('~/.m2:/root/.m2'),
        '-v', '{}/.gitcache:/gitcache'.format(os.getcwd()),
        '-v', '{}/.node_modules:/node_modules'.format(os.getcwd()),
        '-v', '{}/target:/target'.format(os.getcwd()),
        '-v', '{}/scripts:/scripts'.format(os.getcwd()),
        '-t', 'build-navopt',
        '/scripts/build-{}.sh'.format(service),
        args.branch,
    ]
    run_command(args, cmd)


def build_docker_image(args, image):
    print '*'*70
    print 'Building {} Docker image'.format(image)
    print '*'*70

    cmd = [
        'docker',
        'build',
        '-t', image,
        '-f', os.path.join(image, 'Dockerfile'),
        '.',
    ]
    run_command(args, cmd)


def determine_services_to_build(args, services):
    services_set = set()
    services_to_build = []

    def process(service):

        for dependency in SERVICES[service].get('dependencies', ()):
            process(dependency)

        if service in services_set:
            return

        services_set.add(service)

        # If we're not building dependencies, only build the services in our
        # list.
        if args.build_dependencies or service in services:
            services_to_build.append(service)

    for service in args.services:
        process(service.split(":")[0])

    print "will build:"
    print services_to_build

    return services_to_build


def build_services(args, services):
    if not args.build:
        return

    branch_overrides = {}

    for service_override in args.services:
        if ":" in service_override:
            branch_overrides[service_override.split(":")[0]] = service_override.split(":")[1]

    services_to_build = determine_services_to_build(args, services)

    args_default_branch = args.branch

    for service in services_to_build:

        if service in branch_overrides:
            args.branch = branch_overrides[service]

        if SERVICES[service].get('docker', False):
           #  build_docker_image(args, service)
             print("building docker %s branch in  %s service " %  (args.branch, service))
        else:
           #  build_service(args, service)
             print("building %s branch in  %s service " % (args.branch, service))

        args.branch = args_default_branch


def upload_service(args, service, target_bucket):
    """
    Upload the service tarballs.
    """

    for tarball in SERVICES[service]['tarballs']:
        src = os.path.join('target', tarball)

        if not args.dry_run and not os.path.exists(src):
            raise Exception('`{}` does not exist'.format(src))

        dst = 's3://{}/{}'.format(target_bucket, tarball)

        print '--- uploading', src, 'to', dst
        cmd = [
            'aws',
                '--profile', CONFIG[args.env]['aws_profile'],
            's3',
            'cp',
            src,
            dst,
        ]
        print cmd
        run_command(args, cmd)


def refresh_ecs_service(args, service):

    docker_endpoint = '{}.dkr.ecr.{}.amazonaws.com'.format(
        CONFIG[args.env]['account'],
        CONFIG[args.env]['region'],
    )

    cluster = args.cluster
    image = service
    tag = args.tag
    tasks = []

    if 'all' in args.refresh_ecs:
        tasks = SERVICES[service]['ecs_tasks']
    else:
        tasks = args.refresh_ecs

    boto3.setup_default_session(profile_name=CONFIG[args.env]['aws_profile'],\
                                region_name=CONFIG[args.env]['region'])
    try:
        ecs = boto3.client('ecs')
    except ClientError as err:
        print('Failed to create boto3 client.\n%s ... aborting refresh.' % err)
        sys.exit(1)


    for task in tasks:

        task_definition = ecs.describe_task_definition(taskDefinition=task)['taskDefinition']

        task_definition['containerDefinitions'][0]['image'] = '{0}/{1}:{2}'.format(docker_endpoint, \
                                                                                   image, \
                                                                                   tag)

        new_task_definition = ecs.register_task_definition(family = task_definition['family'],\
                        volumes = task_definition['volumes'],\
                        containerDefinitions = task_definition['containerDefinitions'])['taskDefinition']

        sys.stdout.write("Updating service definition for %s > " % (task))

        service_update_thread = FuncThread(ecs.update_service,
                                               kwargs={
                                                   'cluster' : cluster,
                                                   'service' : task,
                                                   'taskDefinition' : task})

        service_update_thread.start()

        while service_update_thread.isAlive():
            sys.stdout.write(".")
            sys.stdout.flush()
            time.sleep(2)
        service_update_thread.join()

        print("")

        sys.stdout.write("(Re)Starting tasks for %s > " % (task))
        sys.stdout.flush()

        # loop for desired timeout
        timeout = time.time() + 90
        task_progress = False
        spawn_progress = False
        while True:
            updated = False
            task_progress = True
            task_arns = ecs.list_tasks(cluster=cluster, family=task)['taskArns']
            # no running tasks for that service ..
            if len(task_arns) == 0:
                if not spawn_progress:
                    sys.stdout.write("Waiting for %s to spawn > " % (task))
                    sys.stdout.flush()
                    spawn_progress = True
                else:
                    sys.stdout.write(".")
                    sys.stdout.flush()
                time.sleep(3)
                continue

            if spawn_progress:
                print ""
                spawn_progress = False

            running_tasks = ecs.describe_tasks(cluster=cluster,tasks=task_arns)['tasks']

            for running_task in running_tasks:
                if running_task['taskDefinitionArn'] == new_task_definition['taskDefinitionArn']:
                    if task_progress:
                        print ""
                    print('SUCCESS %s started' % running_task['taskArn'])
                    task_progress = False
                    updated = True

            if time.time() > timeout:
                print ("Timeout while updating %s... moving on " % (task))
                break
            if updated:
                print ("%s updated . " % (task))
                break
            time.sleep(2)
            sys.stdout.write(".")
            sys.stdout.flush()

        print("")

def push_docker_image(args, image):

    if args.assume_yes is None:
        print
        print '-' * 78
        print
        print \
            'Are you sure you want to push the following image?', \
            image

        if not prompt('[yes/no]: ', args.assume_yes):
            return

    if not args.tag:
        tag = 'latest'
    else:
        tag = args.tag

    if args.assume_yes is None:
        print
        print '-' * 78
        print
        print \
            'Are you sure you want to push the following image?', \
            image

        if not prompt('[yes/no]: ', args.assume_yes):
            return

    # Log into AWS Region.
    # The aws command returns the 'docker login' command.
    # It does not actually run it, so we capture the output
    # of 'aws login' and feed that to run_command().
    cmd = [
        'aws',
        '--profile', CONFIG[args.env]['aws_profile'],
        'ecr',
        'get-login',
        '--region', CONFIG[args.env]['region'],
    ]
    docker_login_cmd = subprocess.check_output(cmd).strip().split(' ')
    run_command(args, docker_login_cmd)

    docker_endpoint = '{}.dkr.ecr.{}.amazonaws.com'.format(
        CONFIG[args.env]['account'],
        CONFIG[args.env]['region'],
    )

    # Tag the image
    cmd = [
        'docker',
        'tag',
        '{0}'.format(image),
        '{0}/{1}:{2}'.format(
            docker_endpoint,
            image,
            tag),
    ]

    run_command(args, cmd)

    # Push the image
    cmd = [
        'docker',
        'push',
        '{0}/{1}:{2}'.format(docker_endpoint, image, tag),
    ]

    #run_command(args, cmd)


def prompt(msg, assume_yes=None):
    if assume_yes is not None:
        return assume_yes

    while True:
        response = raw_input(msg).lower()
        if response in ('yes', 'y'):
            return True
        elif response in ('no', 'n'):
            return False


def upload_services(args, services, target_path):

    if not args.upload or args.assume_yes is False:
        return

    services = set(services)

    target_bucket = CONFIG[args.env]['bucket']

    if args.assume_yes is None:
        print
        print '-' * 78
        print
        print \
            'Are you sure you want to upload the following services?', \
            ' '.join(sorted(services))

        if not prompt('[yes/no]: ', args.assume_yes):
            return

    for service in services:
        service_name = service.split(":")[0]

        if SERVICES[service_name].get('docker', False):
            push_docker_image(args, service_name)
        elif not args.release:
                target_bucket = os.path.join(target_bucket, target_path)

                if ":" in service:
                    target_bucket = os.path.join(target_bucket, service.split(":")[1])
                else:
                    target_bucket = os.path.join(target_bucket, args.branch)

                upload_service(args, service_name, target_bucket)
                target_bucket = ""


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '-v', '--verbose',
        default=False,
        action='store_true',
        help='Print extra debug info')
    parser.add_argument(
        '-n', '--no',
        default=None,
        dest='assume_yes',
        action='store_false',
        help='Answer yes to any questions')
    parser.add_argument(
        '-y', '--yes',
        default=None,
        dest='assume_yes',
        action='store_true',
        help='Answer yes to any questions')
    parser.add_argument(
        '--dry-run',
        default=False,
        action='store_true',
        help='Do not actually run any commands')
    parser.add_argument(
        '--repo',
        default=DEFAULT_GITHUB_PROJECT,
        help='github project to clone from (default {})'.format(
            DEFAULT_GITHUB_PROJECT,
        ))
    parser.add_argument(
        '--env',
        default='dev',
        help='Build for this environment')
    parser.add_argument(
        '--release',
        default=False,
        help='build a release candidate')
    parser.add_argument(
        '--branch',
        default='master',
        help='which common branch to build (default master for all)')
    parser.add_argument(
        '--target_branch',
        default=None,
        help='which branch to build (defaults to `--branch` value)')
    parser.add_argument(
        '--target_path',
        help='upload artifacts to this bucket (default $USERNAME/$BRANCH)')
    parser.add_argument(
        '--artifact-tag',
        dest='tags',
        default=None,
        action='append',
        help='tag artifacts with this tag. may be repeated')
    parser.add_argument(
        '--no-build-dependencies',
        dest='build_dependencies',
        default=True,
        action='store_false',
        help='do not build dependency artifacts')
    parser.add_argument(
        '--no-build',
        dest='build',
        default=True,
        action='store_false',
        help='do not build artifacts')
    parser.add_argument(
        '--no-upload',
        dest='upload',
        default=True,
        action='store_false',
        help='do not upload artifacts to S3')
    parser.add_argument(
        'services',
        default='all',
        #choices=['all'] + SERVICES.keys(),
        nargs='*',
        help='which services to build (service_name:branch)')
    parser.add_argument(
        '--tag',
        default='latest',
        help='tag for the resulting docker image and ecs task')
    parser.add_argument(
        '--refresh-ecs',
        default=None,
        dest='refresh_ecs',
        nargs='*',
        help='refresh task(s) in ecs (complete deploy if applicable)')
    parser.add_argument(
        '--cluster',
        default=None,
        help='cluster to refresh')
    return parser.parse_args()


def main():
    args = parse_args()

    if args.services == 'all' or \
            any(service == 'all' for service in args.services):
        services = SERVICES.keys()
    else:
        services = args.services

    ############################################################################
    # Validate arguments

    # if not args.branch:
    #    print >> sys.stderr, 'build branch is empty'
    #    return 1

    # Use the build branch if the target branch is not specified.
    target_branch = args.target_branch
    if target_branch is None:
        target_branch = args.branch

    if not target_branch:
        print >> sys.stderr, 'target branch is empty'
        return 1

    target_path = args.target_path
    if target_path is None:
        target_path = '{}/'.format(getpass.getuser())

    if not target_path:
        print >> sys.stderr, 'target path is empty'
        return 1

    if args.refresh_ecs:
        if not args.cluster:
            print >> sys.stderr, 'cluster is mandatory with --refresh-ecs'
            return 1

    if args.upload and not args.tag:
        if any(SERVICES[service].get('docker') for service in services):
            print >> sys.stderr, 'docker images require a tag'
            return 1

    ############################################################################
    # Do the build

    if not os.path.exists('.gitcache'):
        os.makedirs('.gitcache')

    if not os.path.exists('.node_modules'):
        os.makedirs('.node_modules')

    if not os.path.exists('target'):
        os.makedirs('target')

    build_services(args, services)
    upload_services(args, services, target_path)

    if args.refresh_ecs:
        for  service in args.services:
            if 'ecs_tasks' in SERVICES[service] and 'docker' in SERVICES[service]:
                print "Refreshing ecs ..."
                refresh_ecs_service(args, service)

if __name__ == '__main__':
    sys.exit(main())
